+ [Background](#background)
  * [datasets](#datasets)
  * [Evaluation Metrics](#evaluation-metrics)
+ [3D shape classification](#3d-shape-classification)
  * [基于多视图的方法](#基于多视图的方法)
  * [基于体积的方法](#基于体积的方法)
  * [基于点云的方法](#基于点云的方法)
    - [Pointwise MLP Methods](#pointwise-mlp-methods)
    - [Convolution-based Methods](#convolution-based-methods)
      + [连续卷积](#连续卷积)
      + [离散卷积](#离散卷积)
    - [Graph-based Methods](#graph-based-methods)
      + [Graph-based Methods in Spatial Domain](#graph-based-methods-in-spatial-domain)
      + [Graph-based Methods in Spectral Domain](#graph-based-methods-in-spectral-domain)
    - [Hierarchical Data Structure-basd Methods](#hierarchical-data-structure-basd-methods)
    - [Other Methods](#other-methods)
  * [Summary](#summary)
+ [3D Object Detection And Tracking](#3d-object-detection-and-tracking)
  * [3D Object Detection](#3d-object-detection)
    - [Region proposal-based Methods](#region-proposal-based-methods)
      + [基于多视图的方法](#基于多视图的方法-1)
      + [基于分割的方法](#基于分割的方法)
      + [Frustum-based方法](#frustum-based方法)
      + [其他方法](#其他方法)
    - [Single Shot Methods](#single-shot-methods)
      + [BEV-baeds Methods](#bev-baeds-methods)
      + [Discretization-based Methods](#discretization-based-methods)
      + [Point-based Methods](#point-based-methods)
      + [Other Methods](#other-methods-1)
    - [3D Object Tracking](#3d-object-tracking)
    - [3D Scene Flow Estimation](#3d-scene-flow-estimation)
    - [Summary](#summary-1)
  * [3D 点云分割](#3d-点云分割)
    - [3D 语义分割](#3d-语义分割)
      + [基于投影的方法](#基于投影的方法)
        * [多视图表示](#多视图表示)
        * [球面表示](#球面表示)
      + [基于离散的方法](#基于离散的方法)
        * [密集离散表示](#密集离散表示)
        * [稀疏离散表示](#稀疏离散表示)
      + [Hybrid Methods](#hybrid-methods)
      + [Point-based Methods](#point-based-methods-1)
        * [Pointwise MLP methods](#pointwise-mlp-methods-1)
        * [Point Convolution Methods](#point-convolution-methods)
        * [RNN-based Methods](#rnn-based-methods)
        * [Graph-based Methods](#graph-based-methods-1)
    - [3D 实例分割](#3d-实例分割)
      + [Propasal-based Methods](#propasal-based-methods)
      + [Propasal-free Methods](#propasal-free-methods)
    - [3D part分割](#3d-part分割)
    - [Summary](#summary-2)
  * [CenterPoint](#centerpoint)
  * [点云](#点云)
    - [点云的内容](#点云的内容)
    - [点云存储格式](#点云存储格式)
    - [点云的数据类型](#点云的数据类型)

# Background
## datasets

## Evaluation Metrics
- 3D shape classification
  * Overall Accuracy(OA):所有instance的平均精度
  * mean class accuracy(mAcc):所有shape classes的平均精度
- 3D object detection
  * Average Precision(AP)
- 3D single object tracker
  * Precision
  * Success
- 3D multi object tracking
  * Average Multi-Object Tracking Accuracy(AMOTA)
  * Average Multi-Object Tracking Precision(AMOTP)
- 3D point cloud segmentation
  * OA
  * mean Intersection voer Union(mIou)
  * mean class Accuracy(mAcc)
  * mAP

# 3D shape classification
- 该任务的方法通常是:
  1. 学习每个point的embedding，
  2. 然后使用聚合方法从整个点云中提取全局shape embedding
  3. 最后将全局shape embedding送入几个fc层得到分类结果
- 根据神经网络输入的数据类型，现有的三维形状分类方法可分为:
  1. 基于多视图：
    a. 将非结构化点云映射到2D图片
    b. 然后通过2D卷积网络实现形状分类
  2. 基于体积：
    a. 将点云转换为3D体积表示
    b. 然后通过3D卷积网络实现形状分类
  3. 基于点云的方法：
    a. 直接处理原始点云数据

  **基于点云的方法直接处理点云数据而不引入显式信息损失，从而变得更常用**。

  详细方法如下图所示
  ![F2](./imgs/3D/F2.png)

## 基于多视图的方法
- 这些方法首先将三维形状投影到多个视图中，提取视图特征，
- 然后融合这些特征以实现精确的形状分类
- 因此**如何将多个视图特征聚合成一个有区别的全局表示**是这些方法面临的一个关键挑战

- 一些代表方法
  * MVCNN
  * MHBN
  * View-GCN
  * ...
## 基于体积的方法
- 这些方法通常将点云体素化为三维网格，
- 然后将三维卷积神经网络（CNN）应用于形状分类的体积表示
- 一些代表方法
  * VoxNet
  * ...
- 虽然这些方法已经取得了令人鼓舞的性能，但这些方法无法很好地扩展到密集的三维数据，因为**计算和内存占用随着分辨率的增加呈立方体增长**。
- 为了降低计算和内存成本，引入了层次结构和紧凑结构(eg. OctNet)
- 一些代表方法
  * OctNet
  * PointGrid
  * 3DmFV

## 基于点云的方法
point-based方法根据网络架构可以分为：
1. pointwise MPL
2. convolution-based
3. graph-based
4. hierarchical data structure-based
5. others

### Pointwise MLP Methods
- 这些方法使用多个共享多层感知器（MLP）对每个点进行独立建模，
- 然后使用对称聚合函数聚合全局特征
- 如下图所示
  ![F3](./imgs/3D/F3.png)
- 代表方法：
  * PointNet
  * PointNet++
  * PointNet-based
    + Mo-Net
    + Point Attention Transformers(PATs)
    + Group Shuffle Attention(GSA)
  * PointNet++-based
    + PointWeb(using Adaptive Feature Adjustment(AFA))
    + Structural Relational Network(SRN)
    + SRINet
    + PointASNL

### Convolution-based Methods
- 与2D网格结构上定义的核相比，三维点云的卷积核由于点云的不规则性而难以设计。
- 根据卷积核的类型，当前的3D卷积方法可分为:
  * 连续卷积方法
  * 离散卷积方法

    如下图所示
    ![F4](./imgs/3D/F4.png)
#### 连续卷积
- 在**连续空间**定义卷积核,
- 其中相邻点的权重与**相对于中心点的空间分布**相关
- 代表方法
  * 3D卷积可以解释为给定子集上的加权和
    + RS-CNN
    + DensePoint
    + ConvPoint
  * 使用现有的方法定义conv
    + PointConv
      + 为了提高内存和计算效率，三维卷积进一步简化为两种运算：**矩阵乘法和二维卷积**。
    + MCCNN
    + SpiderCNN
    + PCNN
  * 几种方法来解决三维卷积网络面临的旋转等变问题
    + 3D Spherical CNN
    + SPHNet(Based on PCNN)
    + Flex-Convolution

#### 离散卷积
- 这些方法在**规则网格**上定义卷积核，
- 其中相邻点的权重与**相对于中心点的偏移**有关
- 代表方法
  * GeoConv
  * PointCNN
  * RIConv
  * A-CNN
  * Rectified Local Phase Volume(ReLPV)
  * SFCNN

### Graph-based Methods
- 基于图的网络将点云中的每个点视为图的顶点，
- 并基于每个点的邻域生成图的有向边, 
- 然后在**空间域(spatial)** 或 **光谱域(spectral)** 中执行特征学习。
- 如下图所示
![F5](./imgs/3D/F5.png)

#### Graph-based Methods in Spatial Domain
- 这些方法是在空间域定义操作(eg. 卷积和池化)
- 代表方法
  * EdgeConditioned Convolution(ECC)
  * DGCNN
  * LDGCNN
  * FoldingNet
  * Dynamic Points Agglomeration Module(DPAM)
  * KCNet
  * G3D
  * ClusterNet
  * Grid-GCN

#### Graph-based Methods in Spectral Domain
- 这些方法将卷积定义为频谱滤波，
- 通过图上的信号与图的拉普拉斯矩阵的特征向量相乘实现
- 代表方法
  * RGCNN
  * AGCN
  * HGNN
- 以上说的方法都是在全局图上操作，还有一些探索局部结构化信息的方法
  * LocalSpecGCN
  * PointGCN
  * 3DTI-Net

### Hierarchical Data Structure-basd Methods
- 这些网络是基于不同的层次数据结构（如octree和kd-tree）构建的。
- 在这些方法中，point特征是沿着树从叶节点到根节点分层学习的。
- 代表方法
  * OctNet
  * Kd-Net
  * 3DContextNet
  * SO-Net
    + 与PointNet++相比，有层次的SOM更有效率并且充分探索了点云的空间分布。

### Other Methods
- RBFNet, 使用稀疏分布的Radial Basis Function(RBF)融合特征。
- 3DPointCapsNet
- PointDAN
- PointAugment
- ShapeContextNet
- RCNet, 使用RNN与2D CNN
  * RCNet-E
- Point2Sequences, RNN-based方法
- PVNet, 学习3D点云和2D图片
  * PVRNet

## Summary
- ModelNet10/40数据集是3D形状分类最常用的数据集
- 下表显示了一些point-based的网络结果对比
![t2.1](./imgs/3D/T2.1.png)
![t2.2](./imgs/3D/T2.2.png)
- 一些发现
  * 为了学习pointwise 特征，Pointwise MLP通常其他网络结构的基础block。
  * 基于卷积的网络可以在不规则3D点云上实现优异的性能。对于不规则数据，离散卷积网络和连续卷积网络都应受到重视。
  * 基于图的网络由于其固有的处理不规则数据的强大能力，近年来越来越受到人们的关注。然而，将spectral domain中基于图的网络扩展到各种图结构仍然具有挑战性。

# 3D Object Detection And Tracking
## 3D Object Detection
- 典型的3D对象检测器将场景的点云作为其输入，并围绕每个检测到的对象生成定向3D边界框，如下图所示。
![F6](./imgs/3D/F6.png)
- 与图像中的目标检测类似，3D目标检测方法可分为两类：
  * region proposal-based方法
  * single shot方法
- 一些里程碑方法如下图所示
![F7](./imgs/3D/F7.png)

### Region proposal-based Methods
- 这些方法首先提出几个可能包含目标的区域(proposals),
- 然后提取区域特征以确定每个proposal的类别标签。
- 根据目标proposals生成方法，可以分为三类：
  * 基于多视图的方法
  * 基于分割的方法
  * frustum-based的方法
#### 基于多视图的方法
- 这些方法然融合来自不同视图maps(eg. 激光雷达前视图、鸟瞰视图（BEV）和2D图像)的区域建议特征。
- 来获得3D旋转边框，如下图所示
![F8.a](./imgs/3D/F8.a.png)
- 但这些方法的计算成本通常很高。
- MV3D
  * 尽管该方法实现了较好的效果，但是速度很慢;
  * 从两个方面可以改进
    + 一些方法提出更有效率的方式融合来自不同方面的信息:
      + AVOD为了提高小目标的recall，提出多模式融合建议区域
      + [Deep continuous..](https://paperswithcode.com/paper/deep-continuous-fusion-for-multi-sensor-3d-1)使用连续卷积在不同的分辨率有效的融合2D图像与3D LiDAR特征图
      + [Multi-task..](https://paperswithcode.com/paper/multi-task-multi-sensor-fusion-for-3d-object-1) 提出Multi-task multi-sensor 3D目标检测框架实现端到端训练
    + 一些方法研究从输入数据中提取更健壮的特征表示:
      + SCANet
        + Spatial Channel Attention(SCA)模块,提取全局和多尺度上下文信息来凸显有用的特征
        + Extension Spatial Unsample(ESU)模块,通过融合多尺度的低层特征获取有更丰富空间信息的高层信息。
        + 尽管取得的效果较好，但是由于需要对每个proposal进行特征pooling，所以上述的多视图方法速度很慢。
      + RT3D 
        + 使用pre-ROI pooling卷积来提高效率, 移除了ROI pooling之前的大部分卷积操作，所以ROI卷积操作可以一次性处理所有的目标proposals。

#### 基于分割的方法
- 这些方法首先利用现有的语义分割技术去除背景points，
- 然后在前景points上生成大量的高质量proposals
- 如下图所示
![F8.b](./imgs/3D/F8.b.png)
- 与基于多视图的方法相比，这些方法能够实现更高的recall，并且更适合包含高度遮挡和拥挤目标的复杂场景。
- 代表方法
  * IPOD
    + 使用2D分割网络去除背景
    + 提出新的标准`PointsIoU`减少冗余和模棱两可的proposals
  * PointRCNN
    + 直接分割3D点云来获取前景points然后融合语义特征和局部空间特征来产生高质量3D边框。
  * PointRGCN
    + 使用PointRCNN中的`RPN`部分
    + 使用GCN
    + 引入两个模块
      + R-GCN利用一个proposal中的所有points实现pre-proposal特征融合
      + C-GCN融合来自所有proposals的preframe信息，利用上下文回归精准的目标框
  * PointPainting
    + 将点云投影到基于2D图像的分割网络的输出中，
    + 并将语义预测分数附加到points上,
    + 再将painted points送入现有的检测器，以实现显著的性能改进
  * STD
    + 将每个point与球形anchor关联
    + 然后使用每个point的语义得分删除冗余anchor
    + 提出了PointsPool层来学习proposals内部point的紧凑特征
    + 引入了并行IOU分支来提高定位精度和检测性能
    + 该方法以更低的计算花费实现了比之前方法更高的recall

#### Frustum-based方法
- 这些方法首先利用现有的2D目标检测器来生成目标的2D候选区域，
- 然后为2D候选区域提取一个3D的frustum proposal。
- 如下图所示
![F8.c](./imgs/3D/F8.c.png)
- 尽管这些方法可以额有效的提取3D目标可能的定位
- 但是该方法很依赖于(受限于)2D的目标检测器的性能
- 代表方法
  * F-PointNets
    + 该类方法的先驱
    + 为2D建议区域生成frustum proposal
    + 采用PointNet(或者PointNet++)为每个3D frustum学习点云特征来估计3D边框
  * Point-SENet
    + 使用注意力机制
    + 使用PointSIFT提取点云的旋转信息
  * PointFusion
    + 利用2D图像区域和相应的frustum points来回归3D边框
    + 提出了一个全局融合网络来直接回归边框的角点位置
    + 提出dense融合网络来预测每个角点的point-wise偏移量
  * RoarNet
    + 首先从2D图片估计目标的2D边框和3D姿态,
    + 然后再提取多个几何上可行的候选框
    + 3D候选框再通过边框回归网络来预测准确的3D目标框
  * Frustun convNet
    + 为每个2D区域沿着frustum axis生成frustums的序列，
    + 然后采用PointNet来提取每个frustum的特征，
    + 对frustum-level特征进行改造，生成二维特征图，然后将其送入全卷积网络进行3D边框估计
    + 该方法在基于2D的方法中实现了SOTA，并在KITTI官方排行中第一
  * Path refinement - localized 3D object detection
    + 首先在BEV map上获取一个初步的检测结果,
    + 然后在BEV的预测上提取小的point子集(patches)
    + 再采用一个refinement网络来学习patches的局部特征来预测精确地3D边框

#### 其他方法
- 受轴对齐IoU在图像目标检测中的成功推动, `iou loss for 2D/3D object detection`将两个3D旋转边界框的IoU集成到多个最先进的检测器，以实现一致的性能改进
- Fast point r-cnn
  * 提出一个two-stage网络架构来使用点云和3维像素表示
  * 首先，对点云进行voxelized，并将其送入3D主干网络，以产生初始检测结果,
  * 然后进一步利用初始预测的interior point特征进行边框优化
  * 尽管该方法简单，但是以16.7的FPS实现了与PointRCNN相当的结果
- PointVoxel-RCNN(PV-RCNN)
  * 利用3D卷积网络和PointNet-based set abstraction来学习点云特征
  * 输入点云首先voxelized，然后送入一个3D稀疏卷积网络来生成高质量的proposals
  * 学习到的voxel-wise特征经过一个voxel set abstraction模块编码为一组关键点
  * 也提出了一个keypoint-to-grid的ROI抽象模块来为边框优化捕捉丰富的上下文信息
  * 实验结果表明，该方法的性能明显优于以前的方法，在KITTI 3D检测基准的汽车类别中排名第一

- VoteNet 
  * 直接从点云中投票目标的虚拟中心点，并通过融合投票特征生成一组高质量的3D目标提案。
  * VoteNet只是用几何信息，明显的比之前的方法效果更好
  * 并在两个大型的outdoor基准(ScanNet,SUN RGB-D)上实现了SOTA
  * 然而，对于部分遮挡的目标，虚拟中心点的预测是不稳定的。
- [Relation graph network..](https://paperswithcode.com/paper/relation-graph-network-for-3d-object) 
  * 在VoteNet的基础上，未解决(缓解)部分遮挡而虚拟中心不稳定的问题
  * 添加了一个方向向量的辅助分支来提高虚拟中心和3D候选框的预测准确率
  * 还建立了一个proposals之间的3D object-object关系图来强调一些更有用处的特征来提高准确率

- ImVoteNet
  * 融合2D目标检测的信息(几何的和语义的信息)到一个3D投票pipeline

- Part-A<sup>2</sup> Net
  * 受3D目标的GT提供了对象内部零件的精确位置这一观察的启发而提出
  * 由part-aware stage和part-aggregation stage组成
  * part-aware stage应用具有稀疏卷积和稀疏反卷积的类似于UNet的网络来学习用于预测和内部目标位置的粗糙生成的point-wise特征。
  * part-aggregation stage 采用ROI-aware pooling来为边框优化融合预测的部分定位

### Single Shot Methods
- 这些方法使用single-shot网络直接预测类别和回归3D bounding boxes,
- 不需要region proposals生成和后处理,
- 因此速度较快
- 根据输入数据的类型，single-shot方法可以分为3各类别：
  * BEV-baeds
  * discretization-based
  * point-based

#### BEV-baeds Methods
- 这些方法使用BEV表示作为输入
- 代表方法
  * PIXOR
    + 对等间距单元场景的点云进行离散化，
    + 再以类似的方式对反射率进行编码，从而得到规则的表示
    + 然后应用FCN估计目标的位置和航向角
    + 该方法由于大部分的single-shot方法(VeloFCN
    ,3D-FCN,Vote3Deep), 并以28.6fps运行
  * HDNET
    + 利用High-Definition(HD) maps的几何和语义的先验信息来提高鲁棒性和检测效果
    + 具体来说，该方法从HD map获得地面点的坐标，然后使用相对于地面的距离进行BEV表示，以弥补道路坡度造成的平移差异。
    + 此外，还将一个二值道路掩码与BEV表示在channel维度拼接来专注于移动物体
    + 由于HD maps并不是无处不在，还提出了一个在线map预测模块，用于估计单个激活雷达(LiDAR)点云估计map先验
    + 这个map-aware方法在TOR4D和KITTI数据集上指标明显高于他的baseline
    + 然而对不同密度的点云的泛化性能较差

  * BirdNet
    + 提出一个归一化map来考虑不同LiDAR传感器的区别
    + 归一化map是一个与BEV map具有相同分辨率的2D网络，它对每个单元格中包含的最大数目的points进行编码。结果表明，该归一化映射显著提高了基于BEV的检测器的泛化能力。

#### Discretization-based Methods
- 这些方法将点云转换为正则离散表示
- 然后使用CNN预测类别和3D边框
- 代表方法
  * [Vehicle detection ...](https://paperswithcode.com/paper/vehicle-detection-from-3d-lidar-using-fully) 
    + 首先提出了使用FCN来做3D目标检测
    + 将点云映射到2D的point map，然后使用2D的FCN来预测边框和置信度
    + 然后将点云离散化到4D(length,width,height,channels)的tensor,
    + 然后扩展2D的FCN-based方法到3D来做3D检测
  * [3D fully convolutional...](https://paperswithcode.com/paper/3d-fully-convolutional-network-for-vehicle) 
    + 相比于上一个方法，该方法准确率提高了20%
    + 但是由于3D卷积和数据的稀疏性，计算资源需求也相应增加
  * Vote3Deep
    + 为了解决3维像素(voxel)的稀疏问题
    + 利用一个以特征为中心的投票体系来为非空的3维像素生成一组投票,
    + 通过计算投票结果获得卷积结果。
    + 计算量是与占用的3维像素量成比例
  * 3D backbone network for 3D object detection
    + 通过堆叠多个稀疏3D CNN构建的一个3D backbone网络
    + 该方法充分利用voxel的稀疏性，节省内存，加快计算速度。
    + 该3D主干网络提取丰富的3D特征用于目标检测，而不会带来沉重的计算负担。

  * VoxelNet
    + voxel-based端到端训练的框架
    + 将点云划分为等间距的voxel，并将每个voxel内的特征编码为4D tensor
    + 然后通过一个region proposal网络来产生检测结果
    + 尽管他的表现很好，但是该方法由于voxels的稀疏性和3D卷积，速度很慢

  * SECOND
    + 使用稀疏卷积网络来提高`VoxelNet`推理的效率
    + 也提出了一个sine-error angle loss来解决角度0和π的相似

  * MVX-Net
    + 通过在较早的stage融合图片和点云特征扩展VoxelNet
    + 将VoxelNet生成的非空voxels与映射到image并使用预训练的网络来为每个voxel提取图像特征。
    + 再将这些图像特征与voxel特征连接起来，从而产生准确的3D边框
    + 与VoxelNet，SECOND相比，该方法可以有效的获取多模式的信息来减少false positive 和negatives。

  * PointPillars
    + 利用PointNet来学习以垂直列(柱)组织点云的特征,并将其编码作为pesudo image
    + 然后采用一个2D的目标检测pipeline来预测3D边框
    + PointPillars比大部分融合方法(包括MV3D,RoarNet,AVOD)精度(AP)高
    + 并且在3D和BEV KITTI基准上以62fps运行

  * SA-SSD
    + 受点云的部分空间信息不可避免地在现有单点检测器逐步缩小的特征图中丢失的观察启发
    + 利用细粒的结构信息来提高定位精度
    + 首先将点云转换为一个tensor，然后给backbone网络来提取multi-stage特征
    + 还提出一个point-level监督的辅助网络来引导特征学习点云的结构
    + 实验证明SA-SSD在KITTI BEV检测基准上汽车类别中排名第一(排行时间2020.6.12)

#### Point-based Methods
- 这些方法直接将原始点云数据作为输入
- 3DSSD是该类方法的先驱工作
  * 为Distance-FPS(D-FPS)和Feature-FPS(F-FPS)引入了一种融合采样策略, 以去除耗时的特征传播(FP)层和PointRCNN中的细化模块。
  * 然后使用一个Candidate Generation(CG)来充分利用代表性的points;
  * 再进一步送入一个有着3D centerness 标签的anchor-free回归头来预测3D目标框;
  * 实验证明3DSSD比two-stage point-based方法PointRCNN精度更高，且有着25fps;


#### Other Methods
- LaserNet
  * 预测每个point的边框概率分布然后融合这些point分布来生成最终的3D 目标框
  * 使用点云的dense Range View(RV)表示作为输入
  * 提出一个快速的mean-shift算法来减少每个point预测的噪声
  * 实现了0-50米的SOTA, 并且运行速度比现有的方法更快

- [Sensor fusion for...](https://paperswithcode.com/paper/sensor-fusion-for-joint-3d-object-detection)  
  * 在LaserNet基础上，使用RGB图像的纹理信息
  * 通过将三维点云投影到二维图像上，将激光雷达point与图像像素相关联，并利用这种关联将RGB信息融合到三维points中。
  * 也考虑3D语义分割作为一个辅助任务来学习更好的表示
  * 方法在保持LaserNet的高效率的同时，在远距离（eg. 50到70米）目标检测和语义分割方面都取得了显著的改进。

- [Object as hotspots..](https://paperswithcode.com/paper/object-as-hotspots-an-anchor-free-3d-object) 
  * 受孤立目标部分上的point可以提供有关目标位置和方向的丰富信息这一观察结果的启发
  * 提出了一个新的Hotspot表示,第一个hotspot-based anchor-free检测器
  * 原始点云数据首先voxelized然后作为backbone网络的输入来产生3D特征图
  * 这些特征图拿来分来hotspots和预测3D边框
  * 实验证明该模型表现良好，且在稀疏点云上有较好的鲁棒性

- Point-GNN
  * 使用GNN处理激光雷达(LiDAR)点云数据，检测3D目标
  * 首先将输入点云编码为具有固定半径的近邻图，
  * 然后将该图输入点GNN以预测对象的类别和方框。

### 3D Object Tracking
- 由于三维目标跟踪可以利用点云中丰富的几何信息，
- 因此有望克服基于图像的跟踪所面临的一些缺点，包括遮挡、照明和尺度变化
- [Leveraging shape ..](https://paperswithcode.com/paper/leveraging-shape-completion-for-3d-siamese) 
  * 受2D目标跟踪中Siamese network的启发,提出一种shape completion regularization的3D Siamese network
  * 首先使用卡尔曼滤波生成候选，然后使用形状正则化将模型和候选对象编码为紧凑表示
  * 再使用预先相似度来搜索下一帧的追踪目标
  * 该方法可以作为目标追踪的一个选项，比大部分2D目标追踪方法效果好(包括STAPLE<sub>CA</sub>和SiamFC)

- [Efficient tracking..](https://arxiv.org/pdf/1903.10168.pdf) 
  * 利用一个2D Siamese网络在BEV表示上生成大量的粗糙的目标候选
  * 然后通过3D Siamese网络的余弦相似度优化校准候选
  * 该方法比上一方法精度高(18%)，成功率高(12%)

- Complexer-YOLO
  * 为语义点云 提出了3D目标检测和跟踪架构
  * 首先通过融合2D是觉得语义信息生成vixelized的语义点云
  * 然后利用时间信息来提高多目标跟踪的精度和鲁棒性
  * 此外，还引入了一个简单有效的评估指标(Scale-Rotation-Translation score (SRFs))来提高训练和推理速度
  * 在较好的跟踪效果的基础上，实现了实时跟踪

- Point-to-Box(P2B)
  * 将时间信息和搜索区域作为backbone的输入来获取seeds
  * 搜索区域seeds通过特定于目标的特征增强，然后通过Hough投票回归潜在目标中心。
  * 实验表明P2B效果好于第一种方法10%，并且以40fps运行

### 3D Scene Flow Estimation
- 给定两个点云X与Y, 3D场景流估计D = {d<sub>i</sub>}<sup>N</sup>就是X中每个点x<sub>i</sub>与相应的Y中的每个点y<sub>i</sub>满足y<sub>i</sub> = x<sub>i</sub> + d<sub>i</sub>
- 如下图所示，展现了两个KITTI点云的3D场景流估计
![F9](./imgs/3D/F9.png) 
- 与2D的光流估计相似，有几种方法已经开始从点云序列中学习有用的信息（如三维场景流、空间临时信息）
- FlowNet3D
  * 从一对连续的点云中直接生成场景流
  * 从flow embedding层学习point-level特征和动作特征
  * 有两个缺点
    + 一些预测动作向量在他们的方向上明显与GT不同
    + FlowNet很难应用于非静态场景，尤其是由可变形目标控制的场景。
    + 为了解决这些问题，提出了FlowNet3D++
- FlowNet3D++
  * 提出了余弦距离损失来最小化预测与GT的角度
  * 也提出了一个point-to-plane的距离损失来提高固定不变的和动态的场景
  * 实验表明这两个损失将FlowNet3D的精度从57.85%提高到63.43%
  * 并且提高了速度和训练的稳定性
- HPLFlowNet
  * 直接从大尺度点云中估计场景流
  * 提出了几个双向卷积层来还原原始点云中的结构信息，并且减少了计算量

- PointRNN, PointGRU, PointLSTM
  * 为了有效的处理点云序列而提出, 用来最终移动的points
  * 可以捕捉时序空间的的信息然后建模动态点云

- MeteorNet
  * 直接从动态点云中学习特征表示
  * 融合相邻points的时空信息
  * 进一步引入直接分组和chained-flow分组来确定时间邻域。
- [Just go with the flow](https://paperswithcode.com/paper/just-go-with-the-flow-self-supervised-scene)
  * 上述的模型的效果受限与数据集的大小
  * 提出了两个自监督损失再大型未标注数据集上训练网络
  * 核心概念是"健壮的场景流估计方法在前向和后向传播应该都是有效的"
  * 由于场景流注释的不可用性，预测变换点的最近邻被视为伪标签
  * 但是GT与伪GT可能存在不同，所以计算了反向的场景流，并提出了循环一致性损失来将点转换到原始位置
  * 实验表明该自监督方法超过了监督方法的SOTA

### Summary
- KITTI基准是自主驾驶领域最具影响力的数据集之一，在学术界和工业界都得到了广泛应用。
- 下面两个表显示了KITTI test 3D基准上不同检测器获得的结果
![T3](./imgs/3D/T3.png) 
![T4](./imgs/3D/T4.png) 
- 一些发现
  * Region proposal-based方法是这两类方法中最常研究的方法，在KITTI test 3D和BEV基准上都比single-shot方法有很大的优势。
  * 先存的3D目标检测有两个局限:
    + long-range的检测能力在现有的方法中是欠缺的
    + 如何全面开发2D图片中的纹理信息还是个问题
  * 多任务学习是3D检测的未来方向，比如MMF通过融合多任务学习了一个跨模态表示来实现SOTA的检测效果
  * 3D目标跟踪和场景流估计是新兴的研究课题，自2019年以来逐渐引起人们的关注。

## 3D 点云分割
- 3D点云分割需要同时理解每个point的几何信息结构和细粒细节
- 根据分割粒度可以分为三类：
  * 语义分割(scene level)
  * 实例分割(object level)
  * part segmentation(part level)

### 3D 语义分割
- 给定一个点云，语义分割的目标是根据point的语义将其划分为若干个子集，与3D形状分类法类似
- 语义分割有四种范式：
  * 基于投影
  * 基于离散化
  * 基于点
  * 混合方法
- 投影和离散化方法的第一步是将点云转换为中间规则表示，例如多视图，球面，体积，permutohedral lattice，混合表示,
- 然后将中间分隔结果投影会原始点云。
- 如下图所示
![F11](./imgs/3D/F11.png) 
- 相反，基于点的方法直接适用于不规则点云
- 几种代表方法如下图所示
![F10](./imgs/3D/F10.png) 

#### 基于投影的方法
- 这些方法通常投影3D点云到2D图片，包括多视图和球形图片
##### 多视图表示
- [Deep projective 3D semantic segmentation](https://paperswithcode.com/paper/deep-projective-3d-semantic-segmentation)
  * 首先从多个虚拟摄像机视图将3D点云投影到2D平面
  * 然后，使用多流FCN预测合成图像上的像素级分数。
  * 通过融合不同视图上的重投影分数，得到每个点的最终语义标签。

- [Unstructured point cloud semantic...](https://paperswithcode.com/paper/unstructured-point-cloud-semantic) 
  * 首先使用多个摄像头位置生成点云的多个RGB和深度快照，然后使用2D分割网络对这些快照执行像素级标记
  * 这些RGB和深度图片的预测分数再通过残差连接进一步融合。

- [Tangent convolution](https://paperswithcode.com/paper/tangent-convolutions-for-dense-prediction-in) 
  * 基于从局部欧几里德曲面采样点云的假设，引入了切线卷积，用于密集点云分割。
  * 此方法首先将每个点周围的局部曲面几何体投影到虚拟切线平面。
  * 然后直接在曲面几何体上操作切线卷积。
  * 该方法具有很强的可扩展性，能够处理具有数百万个点的大规模点云。
- 整体来说，多视图分割方法对视点选择和遮挡非常敏感。
- 此外，由于投影步骤不可避免的引入信息丢失，这些方法没有充分利用潜在的几何和结构信息

##### 球面表示
- SqueezeSeg
  * 为了实现快速和准确的3D点云分割，
  * 基于SqueezeNet和Conditional Random Field(CRF)提出了端到端的网络结构。
- SqueezeSegv2
  * 利用无监督领域适配管道解决域偏移问题，提高精度
- RangeNet++
  * 对于激光雷达点云的实时语义分割
  * 首先将二维距离图像的语义标签转换为三维点云，
  * 然后采用一种基于GPU的KNN后处理方法来缓解离散化误差和模糊推理输出的问题。
  * 与单视图投影相比，球面投影保留了更多的信息，适合于激光雷达点云的标记。
- 然而这种中间表示不可避免地会带来一些问题，如离散化错误和遮挡。

#### 基于离散的方法
- 这类方法通常将点云转换为一个密集/稀疏的离散表示，比如说，体积和稀疏permutohedral lattices
##### 密集离散表示
- 早期的方法通常将点云voxelized为密集网格，然后利用标准的3D的卷积。
- Point cloud labeling using 3D convolutional neural network
  * 首先将点云划分为一组占用体素(voxel)，
  * 然后将这些中间数据提供给全3D 卷积进行体素分割。
  * 最后，为体素内的所有点指定与体素相同的语义标签。
  * 该方法的性能受到体素粒度和点云划分引起的边界伪影的严重限制。
- SEGCloud
  * 实现了细粒度和全局连续的语义分割
  * 引入了一个确定三线插值来将3D-FCNN生成的粗糙的体素预测映射回点云
  * 然后使用全连接CRF(FC-CRF)来对推理出来的逐点标签注入空间连续性

- VV-Net
  * 引入了一个基于核的插值变分自动编码器架构，用于对每个体素内的局部几何结构进行编码,
  * 与二进制占用表示不同，RBFs用于每个体素，以获得连续表示并捕获每个体素中的点分割，
  * VAE进一步用于将每个体素内的点分布映射到紧凑的潜在空间
  * 然后使用对称组和equivalence CNN实现健壮的特征学习
- 由于3D CNN良好的可扩展性，基于体积的网络结构可以使用不同的空间size来训练和测试点云
- Fully-Convolutional Point Network(FCPN)
  * 首先从点云中分层提取不同层次的几何关系，
  * 然后使用3D卷积和加权平均池来提取特征并合并长期依赖关系。
  * 该方法可以处理大规模的点云，并且在推理的时候有良好的扩展性

- ScanComplete
  * 实现3D扫描完成和逐体素语义标记。
  * 该方法利用全卷积神经网络的扩展性，并在训练和测试的时候采用不同的输入data sizes
  * 提出一种"由粗到细"的策略来分层提高预测结果的清晰度(or 分辨率)
- 整体来说，体积表示自然保留了3D点云的领域结构,
- 其常规数据格式还允许直接应用标准3D卷积,
- 这些因素导致该领域的稳步提高。
- 然而体素化步骤固有的引入了离散化伪影和信息丢失。
- 通常高分辨率会导致高内存和计算成本，而低分辨率会导致细节丢失

##### 稀疏离散表示
- 体积表示自然是稀疏的，因为非零值的数量只占很小的百分比。
- 因此，将密集卷积神经网络应用于空间稀疏数据是低效的。
- [3D semantic segmentation with submanifold sparse...](https://paperswithcode.com/paper/3d-semantic-segmentation-with-submanifold) 
  * 为了解决上述的低效问题而提出了基于index结构的子流形稀疏卷积网络
  * 该方法通过限制卷积输出仅与占用的体素相关，显著降低了内存和计算成本
  * 同时，该稀疏卷积也可以控制提取特征的稀疏性
  * 该子流形稀疏卷积非常适合有效的处理高维空间数据

- [MinkowskiNet](https://paperswithcode.com/paper/4d-spatio-temporal-convnets-minkowski) 
  * 为3D视频感知提出一个4D时空卷积神经网络
  * 提出了一个通用的稀疏卷积来有效的处理高维度的数据
  * 使用trilateral-stationary条件随机场来增强一致性

- Sparse Lattice Network(SPLATNet)
  * 基于Bilateral Convolution layers(BCLs)
  * 首先将原始点云数据插值到准正六面体稀疏晶格上,
  * 然后采用BCL对稀疏填充晶格的占用部分进行卷积,
  * 再将过滤后的输出插值回原始点云。
  * 此外，该方法允许灵活的联合处理多视图图片和点云。

- LatticeNet
  * 对大点云实现有效的处理
  * 引入了一个叫做DeformsSlice的依赖数据的插值模块, 来将lattice特征映射回点云

#### Hybrid Methods
- 为了利用所有有效的信息，一些方法提出从3D扫描中学习多模态特征。
- 3DMV
  * 使用一个联合3D多视图网络来融合RGB特征和几何特征
  * 使用一个3D CNN流和几个3D流来提取特征，
  * 提出一个不同的反映射层来联合融合学习到的2D特征编码和3D几何特征
- [A unified point-based framework for 3D segmentation](https://paperswithcode.com/paper/a-unified-point-based-framework-for-3d)
  * 提出一个通用的point-based框架从点云中学习2D纹理外观，3D结构和全局上下文特征信息
  * 该方法直接从稀疏采样的点集中利用point-based网络提取局部几何特征和全局上下文信息，没有体素化的操作
- MVPNet
  * 融合2D多视图图片中外观特征和规范点云中的空间几何特征

#### Point-based Methods
- Point-based网络直接处理不规则的点云，然而点云是无序且不规则的，因此不能直接用标准的CNNs。
- PointNet提出 使用共享的MLPs学习每个point的特征，使用对称的池化来学习全局特征。
- 基于PointNet, 也提出了一系列相关的工作，这些工作可以分为：
  * pointwise MPL Methods
  * point convolution Methods
  * RNN-based Methods
  * graph-based Methods

##### Pointwise MLP methods
- 这些方法通常是使用共享MLP作为网络结构的基础单元，得到较高的效率
- 然而从共享MLP学习到的point-wise特征，缺乏点云中的局部几何信息和交互信息。
- 为了捕获每个点云更丰富的上下文信息和局部几何信息，引入了一些专注于这方面的网络：
  * neighboring feature pooling
  * attention-based aggregation
  * local-global feature concatenation

**Neighboring feature pooling**
- 为了捕获局部几何模块，这些方法对于每个point，融合了其局部邻近的points来学习特征
- PointNet++
  * 按层次和渐进方式将points分组, 并逐步从较大的局部区域学习
  * 如下图所示
![F12.a](./imgs/3D/F12.a.png) 
- 多尺度分组和多分辨率分组来克服点云的不对称和多变性
- PointSIFT
  * 实现方向编码和尺度感知
  * 该模块通过三级有序卷积对来自八个空间方向的信息进行堆叠和编码，
  * 连接多尺度的特征来实现对不同尺度的自适应
- [Know what your neighbors do...](https://paperswithcode.com/paper/know-what-your-neighbors-do-3d-semantic)
  * 利用k-means聚类和KNN来分开同一个空间的两个neighbor
  * 基于同一类的点在特征空间中更接近的假设，引入两两距离损失和质心损失来进一步正则化特征学习。
- PointWeb
  * 为了模拟不同point之间的相互作用
  * 通过密集构建局部fully-linked web来探索局部区域中所有点对之间的关系。 
  * 提出一个Adaptive Feature Adjustment(AFA)模块来实现信息交换和特征校准。
  * 此聚合操作有助于网络学习具有区分性的特征表示
- ShellNet
  * 基于同心球体shells的统计数据，提出了一种叫做Shellconv的置换不变卷积。
  * 该方法首先查询一组多尺度同心球体，然后在不同的shells内使用最大池化来汇总统计信息
  * 然后使用MLP和一维卷积来获得最终的卷积输出
- RandLA-Net
  * 为大规模点云分割提出了一个有效且轻量的网络，RandLA-Net
  * 该网络利用随机point采样，在内存和计算方面实现了非常高的效率。
  * 进一步提出了一种局部特征聚合模块来提取和保护几何特征

**Attention-based aggregation**
- 为了提高分割准确率，在点云分割中引入了注意力机制
- [Modeling point clouds with self-attention...](https://paperswithcode.com/paper/modeling-point-clouds-with-self-attention-and) 
  * 提出了一种group shuffle注意力来建模points之间的关系
  * 提出了一个permutation-invariant、task-agnosic、可微的Gumbel Subset Sampling(GSS)来代替常用的FPS方法
  * 该模块对异常值不太敏感，可以选择具有代表性的点子集。
- Local Spatial Aware(LSA)
  * 为了更好的捕获点云的空间分布
  * 基于点云的空间布局和局部结构来学习空间相关的权重
- Attention-based Score Refinement(ASR)
  * 该模块再次处理网络的输出分割结果
  * 初始分割结果通过将相邻点的分数与学习到的注意权重相结合来细化。
  * 该模块可以很容易地集成到现有的深度网络中，以提高分段性能。

**Local-global concatenation**
- PS<sup>2</sup>-Net
  * 合并点云的局部结构信息和全局上下文信息
- Edgeconv和NetVLAD
  * 重复的堆叠来捕获局部信息和全局特征

##### Point Convolution Methods
- 这些方法为处理点云提出有效的卷积操作
- [Pointwise convolutional neural networks](https://paperswithcode.com/paper/pointwise-convolutional-neural-networks)
  * 提出了一种逐点卷积算子，将相邻点合并到核单元中，然后与核权重进行卷积
- PCCN
  * 提出了一种基于参数连续卷积层的PCCN网络
  * 该层的核函数由MLPs参数化，跨越连续向量空间
  * 如下图所示
![F12.b](./imgs/3D/F12.b.png) 

- Kernel Point Fully Convolution Network(KP-FCNN)
  * 基于Kernel Point Convolution(KPConv)
  * KPConv的权重由到核点的欧几里得距离确定，并且核点的数目不是固定的。
  * 该核点的位置表示为球面空间中最佳覆盖的优化问题
  * 注意，半径邻域用于保持一致的感受野，而网格子采样用于每层，以在不同密度的点云下实现高鲁棒性

- [Dilated point convolutions...](https://paperswithcode.com/paper/dilated-point-convolutions-on-the-receptive)
  * 提供了丰富的消融实验和可视化结果，以显示感受野对聚集方法性能的影响
  * 提出了Dilated Point Convlution(DPC)操作来融合膨胀的相邻特征, 而不是K个最近的neighbours

##### RNN-based Methods
- 为了从点云中捕获内在的上下文特征，RNN方法也使用在点云语义分割中
- [Exploring spatial context for 3D ...](https://paperswithcode.com/paper/exploring-spatial-context-for-3d-semantic)
  * 该方法是基于PointNet的
  * 首先将点云block转换为多尺度的blocks和网格blocks，从而获得input-level context
  * 然后PointNet提取的block-wise特征按顺序的送入Consolidation Units(CU)和Recurrent Consolidation Units(RCU)。从而获取output-level context
  * 实验证明，合并的空间上下文信息对提高分割表现很重要

- [Recurrent slice networks for 3D segmentation of point clouds](https://paperswithcode.com/paper/recurrent-slice-networks-for-3d-segmentation)
  * 提出了一个轻量的局部依赖建模模块，并利用一个切片池化层来将无序的point特征集转化为有序的特征向量序列

- [3D recurrent neural networks...](https://paperswithcode.com/paper/3d-recurrent-neural-networks-with-context)
  * 首先提出了一个Pointwise Pyramid Pooling(3P)模块来捕获由粗到细的局部结果，
  * 然后利用双向RNNs来进一步提起long-range的空间依赖。
  * 如下图所示
![F12.c](./imgs/3D/F12.c.png)

- 但是这些方法在融合局部相邻特征与全局结构特征时，丢失了点云丰富的几何特征和密度分布
- DARNet
  * 为了缓解刚性和静态池操作所带来的问题
  * 同时考虑全局场景复杂性和局部几何特征
  * 使用自适应感受野和节点权重动态聚合inter-medium特征。
- 3DCNN-DQN-RNN
  * 首先使用3D CNN学习颜色特征和空间分布
  * 然后使用DQN进一步来定位属于特定类别的对象。
  * 最后将拼接后的特征向量送入残差RNN，得到最终的分割结果

##### Graph-based Methods
- 为了捕捉三维点云的基本形状和几何结构，有些工作使用了图神经网络
- [Large-scale point cloud semantic...](https://paperswithcode.com/paper/large-scale-point-cloud-semantic-segmentation)
  * 将点云表示为一组相互关联的简单形状和超点，并使用属性有向图（即超点图）来捕获结构和上下文信息。
  * 然后，将大规模点云分割问题分解为三个子问题，即几何齐次分割、重叠点嵌入和上下文分割

- [Point cloud oversegmentation...](https://paperswithcode.com/paper/point-cloud-oversegmentation-with-graph)
  * 提出了一个监督框架，将点云过渡分割为纯超点
  * 该问题被描述为由邻接图构成的深度度量学习问题
  * 此外，论文中还提出了一种图形结构的对比损失来帮助识别物体之间的边界

- PyramNet
  * 为了更好的捕获高维空间的局部几何关系
  * 基于Graph Embedding Module(GEM)和Pyramid Attention Network(PAN)
  * GEM模块将点云表示为有向无环图，并使用协方差矩阵代替欧氏距离来构造相邻的相似矩阵
  * 在PAN中使用四中不同kernel size的卷积，来提取不同语义强度的特征
- GAC(Graph Attention Convolution)
  * 提出GAC来有选择的从局部邻近集中学习正确的特征
  * 该操作通过基于不同相邻点和特征通道的空间位置和特征差异动态地分配注意力权重来实现。
  * GAC可以为分割学会捕捉有辨别力的信息，并且与CRF模型有相似的特征

- PointGCR
  * 使用无向图表示从通道维度捕获全局上下文信息
  * PointGCR是一个即插即用和端到端可培训模块。
  * 它可以很容易地集成到现有的细分网络中，以实现性能改进

- 此外还有些工作尝试使用弱监督来做点云语义分割
  * [Multi-path region mining for weakly...](https://paperswithcode.com/paper/multi-path-region-mining-for-weakl)
    + 提出一个two-stage方法在subcloud level labels上训练一个分割网络
  * [Weakly supervised semantic point cloud...](https://paperswithcode.com/paper/weakly-supervised-semantic-point-cloud-1)
    + 研究了几种用于点云语义分割的不精确监督方案。
    + 还提出了一个网络，该网络只需部分标注的points(eg. 10%)训练即可

### 3D 实例分割
- 现有的3D实例分割可以分为两类：
  * proposal-based Methods
  * proposal-free methods
  * 一些里程碑的方法如下图所示
![F13](./imgs/3D/F13.png)

#### Propasal-based Methods
- 这些方法将实例分割转换为两个子任务：
  * 3D目标检测
  * 实例mask预测
- 3D-SIS
  * 提出了一个3D全卷积实例分割网络，在RGB-D扫描上实现实例分割
  * 同时学习颜色和几何特征
  * 与3D目标检测相同，使用了3D-RPN和3D-RoI层来预测边框定位、类别和mask

- GSPN(Generative Shape Proposal Network)
  * 提出了GSPN来生成高置信度的3D proposals
  * 然后使用Region-based PointNet(R-PointNet)进一步校准这些proposals
  * 通过预测每个类标签的每点二进制掩码来获得最终标签。
  * 与从点云直接回归三维边框不同，该方法通过强制几何理解去除大量无意义的建议。

- PanopaticFusion
  * 通过将二维全景分割扩展到三维映射
  * 提出了在线三维立体映射系统，以共同实现大规模三维重建、语义标记和实例分割
  * 首先利用2D语义和实例分割网络获得像素级全景标签，然后将这些标签集成到体积图中
  * 再使用全连接CRF来进一步实现精确的分割

- 3D-BoNet
  * single-stage, anchor-free, end-to-end
  * 该方法直接回归所有潜在实例的粗糙三维边界框，然后利用point-level二元分类器获得实例标签。
  * 将边框生成任务看作是一个优化分配问题
  * 提出了一种multi-criteria损失函数来正则化生成的边框。
  * 该方法不需要任何后处理，计算效率高

- Instance segmentation of lidar point clouds
  * 使用self-attention block学习点云鸟瞰图上的特征表示
  * 最终实例标签是基于预测的水平中心和高度限制获得的。
- VDRAE(Variational Denoising Recursive AutoEncoder)
  * 提出了VDRAE用于预测室内3D空间的布局
  * 目标建议通过递归上下文聚合和传播迭代生成和细化

- 总体而言，基于proposals的方法简单直接，实例分割结果通常具有良好的客观性。然而，这些方法需要多阶段的训练和冗余建议的删减。因此，它们通常计算量很大且非常耗时

#### Propasal-free Methods
- Proposal-free方法没有目标检测模块
- 该方法通常将实例分割看作语义分割后的后续聚类步骤
- 现存的大部分方法都是基于**同一个实例的点应该具有非常相似的特征**的假设
- 因此，这些方法主要集中在**鉴别特征学习和点分组**上

- SGPN(Similarity Group Proposal Network)
  * 该方法首先为每个point学习一个特征和语义map
  * 然后引入一个相似矩阵来表示每对特征的相似度
  * 为了学习更多的鉴别性特征，使用了一个double-hinge loss来相互调整相似矩阵和语义分割结果
  * 最后，采用nms将相似点合并到实例中。
  * 但是建立相似矩阵需要大量的内存，该方法的扩展性也受限于此
  -MASC
  * 首先利用submanifold稀疏卷积来预测每个体素的语义信息和邻近体素之间的关系
  * 然后引入了一个聚类算法基于预测的关系和mesh topology将points分组成实例

- PartNet
  * 引入了一个detection-by-segmentation网络来实现实例分割
  * PointNet++作为backbone来为每个point预测不相交实例掩码的语义标签

- [3D graph embedding learning with a...](https://paperswithcode.com/paper/3d-graph-embedding-learning-with-a-structure)
  * 提出一个structure-aware loss来学习鉴别性的编码
  * 该loss同时考虑特征之间的相似性和点之间的几何关系
  * 采用一个attention-based graph CNN通过融合邻近点的不同信息来进一步自适应的校准学习的特征
- 由于点的语义类别和实例标签通常相互依赖，一些方法提出将两个任务融合成一个任务
- ASIS(Associatively Segmenting Instances and Semantics)
  * 引入了一个end-to-end和可学习的AIS模块

- JSNet
- MT-PNet(Multi-Task Point-wise Network)
  * 为每个点分配一个标签，并通过引入判别损失来规范特征空间中的嵌入
  * 然后将预测的语义标签和嵌入融合送入一个Multi-Value Conditional Random Field(MV-CRF)模型做联合优化
  * 最后利用mean-field variational推理产生语义标签和实例标签。
- DRG(Dynamic Region Growing)
  * 将点云动态分离为一组不相交的patches，然后使用无监督Kmeans++算法对所有patches进行分组
  * 然后在patches之间上下文信息的指导下，进行多尺度patch分割。
  * 最后，将这些标记的patch合并到object级别，以获得最终的语义和实例标签。

- 3D-BEVIS
  * 提出混合的2D-3D网络来从BEV表示和点云的局部几何特征联合学习全局连续实例特征
  * 然后融合学习的特征来实现语义和实例分割
  * 使用了更灵活的Mean-shift算法来将这些points分组成实例

- [3D instance segmentation via...](https://paperswithcode.com/paper/3d-instance-segmentation-via-multi-task)
  * 学习了每个实例的独特特征嵌入和方向信息来估计对象的中心
  * 提出Feature embedding loss和directional loss来调整在潜在特征空间学习到的特征嵌入
  * 采用Mead-shift聚类和nms来将体素分组为instance
  * 该方法在ScanNet基准上实现了SOTA的表现
  * 除此之外，预测出的方向信息对于决定instance的边界也是很有用的
- [Point cloud instance segmentation using probabilistic embeddings](https://paperswithcode.com/paper/point-cloud-instance-segmentation-using)
  * 引入了一个概率嵌入来做点云实例分割
  * 该方法还结合了不确定性估计，并为聚类步骤提出了一个新的损失函数。
- PointGroup
  * 提出了一种由语义分割分支和偏移量预测分支组成的PointGroup网络。
  * 进一步利用双集聚类算法和ScoreNet实现更好的分组结果

- 总的来说，proposal-free方法不需要昂贵的区域建议组件, 然而，这些方法分组的实例段的对象性通常很低，因为这些方法没有显式地检测目标边界

### 3D part分割
- 3D part segmentation的难点有两个：
  - 具有相同语义标签的形状零件具有较大的几何变化和模糊性。
  - 具有相同语义的目标中的部分数量可能不同。

- VosSegNet
  * 提出在有限的解决方案下实现三维体素化数据的细粒度零件分割
  * 提出SDE(Spatial Dense Extraction)模块从稀疏体素数据中提取多尺度鉴别性的特征
  * 进一步使用一个AFA(Attention Feature Aggregation)模块re-weighted并融合学习到的特征

- [3D shape segmentation with projective convolutional networks](https://paperswithcode.com/paper/3d-shape-segmentation-with-projective)
  * 融合FCNs和surface-based CRFs来实现end-to-end的3D part segmentation
  * 首先从多个视图生成图像以实现最佳表面覆盖，并将这些图像输入2D网络以生成置信度map。
  * 然后，这些置信度map由基于曲面的CRF聚合，该CRF负责对整个场景进行一致的标记

- SyncSpecCNN
  * 用于对不规则和非同构形状图执行卷积。
  * 提出了一种膨胀卷积核的谱参数化方法和谱变换网络，解决了零件多尺度分析和形状信息共享问题
- SFCN(Shape Fully Convolutional Networks)
  * 在3D mesh上进行形状分割
  * 将三个low-level的几何特征作为输入
  * 然后利用基于投票的多标签图切割进一步细化分割结果。
- CoSegNet
  * 提出弱监督方法做3D形状协同分割
  * 网络将未分段的三维点云形状集合作为输入，并通过迭代最小化组一致性损失来生成形状零件标签
  * 与CRF类似，提出了一个预先训练的零件优化网络，以进一步优化和消除零件方案中的噪声。
- BAE-NET(Branched AutoEncoder network)
  * 为无监督，one-shot和弱监督3D形状协同分割而提出的
  * 该方法将形状协同分割任务看做是一个特征表示学习问题，目的是通过最小化形状重建损失找出最简单的part表示
  * 基于encoder-decoder架构，该网络的每个分支可以为一个特别的part形状学习一个紧密的表示
  * 每个分支学习到的点候选然后送入decoder来产生二进制值
  * 该方法具有很好的泛化性，并且可以额处理大型的3D形状集合(5000+shapes)
  * 然而，该方法对初始参数敏感，并且没有将形状语义融入到网络中，这阻碍了该方法在每次迭代中获得鲁棒和稳定的估计
- PartNet
  * 提出了一种自上而下的递归零件分解网络（PartNet），用于分层形状分割
  * 与现有的将形状分割为固定标签集的方法不同，该网络将零件分割描述为级联二值标签问题，并根据几何结构将输入点云分解为任意数量的零件。
- [Learning to group](https://paperswithcode.com/paper/learning-to-group-a-bottom-up-framework-for-1)
  * 为zero-shot 3D part分割引入了一个learning-based分组框架
  * 为了提高跨类别泛化能力, 该方法倾向于学习一个策略，该策略限制网络在零件局部上下文中学习零件级特征

### Summary
- 下表展示了一些现有的方法在一些基准(包括S3DIS,Semantic3D,ScanNet,SemanticKITTI)上的结果
![T5](./imgs/3D/T5.png)
- 得益于规则数据表示，projection-based和discretization-based方法可以利用其2D图像对应的成熟网络架构; 但是projection-based方法的主要受限于3D到2D映射时的信息损失; discretization-based方法的瓶颈是分辨率的提高导致计算和存储成本的增加;为此，基于索引结构的稀疏卷积将是一个可行的解决方案，值得进一步探索
- Point-based是最常用的方法, 但是点表示自然是没有显式邻近信息的; 大多数现有的point-based的方法都求助于昂贵的邻居搜索机制（例如，KNN或ball查询); 这限制了这些方法的效率，最近提出的点体素关节表示法将是进一步研究的一个有趣方向。
- 在点云分割领域，从不平衡的数据中学习依旧是一个挑战; 尽管有一些方法实现了很好的表现，但是他们在少数类别中效果还是受限；比如说RandLA-Net在Semantic3D的`reduced-8`子集上实现了76.0%的IoU，但是在`hardscape`类别中实现的IoU很低(41.1%)。
- 大多数现有的方法是在小型点云(1mx1m, 4096 points)上工作的；实际上，深度传感器获取的点云通常是巨大的、大规模的。因此，需要进一步研究大规模点云的有效分割问题。
- 少数工作已经开始从动态点云中学习时空信息。预计时空信息有助于提高后续任务的性能，如3D对象识别，分割和补全。

-----------
## CenterPoint
与2D检测不同的挑战
1. 点云是稀疏的，并且大部分3D空间的regions是没有测量的
2. 生成的输出是一个三维长方体，该长方体通常与任何全局坐标系都不对齐
3. 3D目标的大小、形状和纵横比范围广泛，例如，在交通领域，自行车靠近刨床，公交车和豪华轿车是细长的，行人是高的。

-----------
## 点云
### 点云的内容
- 根据激光测量原理得到的点云，包括三维坐标（XYZ）和激光反射强度（Intensity）
  * 强度信息与目标的表面材质、粗糙度、入射角方向，以及仪器的发射能量，激光波长有关
- 根据摄影测量原理得到的点云，包括三维坐标（XYZ）和颜色信息（RGB）
- 结合激光测量和摄影测量原理得到点云，包括三维坐标（XYZ）、激光反射强度（Intensity）和颜色信息（RGB）
### 点云存储格式
- \*.pts
- \*.asc
- \*.dat
- \*.stl
- \*.imw
- \*.xyz
- \*.las
  * LAS格式文件已经成为LiDAR数据的工业标准格式

### 点云的数据类型
1. pcl::PointCloudpcl::PointXYZ
PointXYZ 成员：float x，y，z;表示了xyz3D信息，可以通过points[i].data[0]或points[i].x访问点X的坐标值
2. pcl::PointCloudpcl::PointXYZI
PointXYZI成员：float x, y, z, intensity; 表示XYZ信息加上强度信息的类型。
3. pcl::PointCloudpcl::PointXYZRGB
PointXYZRGB 成员：float x,y,z,rgb; 表示XYZ信息加上RGB信息，RGB存储为一个float。
4. pcl::PointCloudpcl::PointXYZRGBA
PointXYZRGBA 成员：float x , y, z; uint32_t rgba; 表示XYZ信息加上RGBA信息，RGBA用32bit的int型存储的。
5.  PointXY 成员：float x,y;简单的二维x-y点结构
6. Normal结构体：表示给定点所在样本曲面上的法线方向，以及对应曲率的测量值，用第四个元素来占位，兼容SSE和高效计算。
